{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Series 1: Data Preprocessing blblblblblbblaaa" 
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conceptual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. understanding td-idf\n",
    "\n",
    "Consider tf-idf score for word i and document j of a corpus. Which of the following statements are correct and which ones are false? Justify your answers.\n",
    "\n",
    "a) tf-idf is lower if word i occurs in many documents\n",
    "\n",
    "b) tf-idf is higher if word i occurs in many documents\n",
    "\n",
    "c) tf-idf does not count how often word i occurs in documents other than document j\n",
    "\n",
    "d) tf-idf does not count how often word i occurs in document j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. dealing with missing values\n",
    "\n",
    "The soybean data can also be found at the [UC Irvine Machine Learning\n",
    "Repository](https://archive.ics.uci.edu/ml/datasets/Soybean+(Large%29). Data were collected to predict disease in 307 soybeans. The 35 features are mostly categorical and include information on the environmental conditions (e.g., temperature, precipitation) and plant conditions (e.g., left spots, mold growth). The outcome labels consist of 19 distinct classes.\n",
    "\n",
    "(a) what percentage of samples have missing data? \n",
    "\n",
    "(b) are there particular features that are more likely to be missing? \n",
    "\n",
    "(c) is the pattern of missing data related to the outcome classes?\n",
    "\n",
    "(d) in case of not having an expert to fill out missing values, discuss which strategy is more feasible to deal with samples including missing values: (i) sample elimination, (ii) value imputation, and (iii) a hybrid of both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. data exploration and outlier detection\n",
    "\n",
    "The UC Irvine Machine Learning Repository contains a [data set related to glass identification]( https://archive.ics.uci.edu/ml/datasets/Glass+Identification ). The data consist of 214 glass samples labeled as one of seven glass categories. There are nine features, including the refractive index and percentages of eight elements: Na, Mg, Al, Si, K, Ca, Ba, and Fe.\n",
    "\n",
    "(a) using visualizations, explore the features to understand their distributions. \n",
    "\n",
    "(b) using visualizations, explore the pairwise relationships between features.\n",
    "\n",
    "(c) calculate the correlation of each feature with outcome class.\n",
    "\n",
    "(d) considering feature distributions, do there appear to be any outliers in the data? \n",
    "\n",
    "Hint: consider take advantage of pairplot facility of seaborn visualization library. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. data transformation techniques\n",
    "\n",
    "Use different methods to transform the following values of feature 'age':\n",
    "\n",
    " 13, 15, 16, 16, 19, 20, 20, 21, 22, 22, 25, 25, 25, 25, 30, 33, 33, 35, 35, 35, 35, 36, 40, 45, 46, 52, 70\n",
    "\n",
    "(a) min-max standardization by setting min = 0 and max = 1\n",
    "\n",
    "(b) z-score standardization\n",
    "\n",
    "(c) normalization using l1 and l2 norms\n",
    "\n",
    "(d) partition data into 3 buckets using equal-frequency and equal-width partitioning and labels each category to 'young', 'middle' and 'old'\n",
    "\n",
    "(e) encode the categorical feature obtained in part (d) using one hot encoding technique "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. feature extraction of text documents using bag of words\n",
    "\n",
    "Consider the following corpus:\n",
    "\n",
    "documents = [\"Human machine interface for lab abc computer applications\",\n",
    "             \"A survey of user opinion of computer system response time\",\n",
    "             \"The EPS user interface management system\",\n",
    "             \"System and human system engineering testing of EPS\",              \n",
    "             \"Relation of user perceived response time to error measurement\",\n",
    "             \"The generation of random binary unordered trees\",\n",
    "             \"The intersection graph of paths in trees\",\n",
    "             \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "             \"Graph minors A survey\"]\n",
    "             \n",
    "Using sklearn feature extraction facilities, apply the followings:\n",
    "\n",
    "(a) tokenize the corpus by applying normalization and stop word removal (use  [feature_extraction.text.CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)).\n",
    "\n",
    "(b) obtain the tf-idf value for each token of each sentence in the corpus (use [sklearn.feature_extraction.text.TfidfTransformer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html)).\n",
    "\n",
    "(c) create a simple pipeline to apply both steps at once (use [sklearn.pipeline.Pipeline](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)).\n",
    "\n",
    "NOTE: before doing this exercise, try to understand how the mentioned classes are working using given sklearn documentations and examples. In general understanding how to use classes and functions that you never worked with before using their sklearn documentations and examples (as well as additional web resources in case) is an important skill that you should master. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6. stemming of text documents\n",
    "\n",
    "Feature extraction facilities of sklearn do not provide stemming. Modify your pipeline in the previous question to incorporate stemming facilities of nltk (use nltk.stem.snowball.EnglishStemmer) and regenerate vectorized form of the corpus with new features and tf-idf values. \n",
    "\n",
    "Hint: you can configure your CountVectorizer object as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 0)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 4)\t1\n",
      "['come', 'great', 'power', 'respons', 'with']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "\n",
    "stemmer = EnglishStemmer()\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "stem_vectorizer = CountVectorizer(analyzer=stemmed_words)\n",
    "print(stem_vectorizer.fit_transform(['with power comes great responsibilities']))\n",
    "print(stem_vectorizer.get_feature_names())"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
